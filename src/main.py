import streamlit as st
from dotenv import load_dotenv
from seed_data import seed_milvus
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_classic.memory import StreamlitChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from agent import (get_retriever as get_openai_retriever, 
                   get_llm_and_agent as get_openai_agent)
from ollama_local import (get_retriever as get_ollama_retriever, 
                          get_llm_and_agent as get_ollama_agent)
import os
import gc
import torch

os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'

def release_memory():
    if torch.cuda.is_available():
        gc.collect()
        torch.cuda.empty_cache()
        if hasattr(torch.cuda, 'reset_peak_memory_stats'):
            torch.cuda.reset_peak_memory_stats()


def setup_page():
    st.set_page_config(
        page_title="AI Assistant", 
        page_icon="üí¨",  
        layout="wide"  
    )

def initialize_app():
    load_dotenv()
    setup_page()

def setup_sidebar():
    with st.sidebar:
        st.title("Configuration")

        # Chose embedding model
        st.header("Embeddings Model")
        embedding_choice = st.radio(
            "Select Embedding Model:",
            ["viHuggingFace Embeddings", "OpenAI Embeddings"]
        )

        use_vihuggingface_embeddings = ( embedding_choice == "viHuggingFace Embeddings" )

        # Config data
        data_source = st.selectbox(
            "Select Data Source:",
            ("Upload Handbook", "File local")
        )

        if data_source == "Upload Handbook":
            handle_upload_file(use_vihuggingface_embeddings)
        else:
            handle_local_file(use_vihuggingface_embeddings)

        st.header("Collection to query")
        collection_to_query = st.text_input(
            "Enter collection name stored Milvus: ",
            "student_handbook",
            help="Nh·∫≠p t√™n collection b·∫°n mu·ªën s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin"
        )

        # Model to answer 
        st.header("Model AI")
        model_choice = st.radio(
            "AI model to answer:",
            ["Qwen3-4B-Instruct (local)", "OpenAI GPT-5-nano"]
        )
        return model_choice, collection_to_query

def handle_upload_file(use_vihuggingface_embeddings: bool):
    collection_name = st.text_input(
        "Collection name to save in Milvus: ",
        "student_handbook",
        help="Nh·∫≠p t√™n collection ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu trong Milvus",
    )

    # File uploader
    uploaded_file = st.file_uploader(
        "Upload file",
        type=["json", "pdf"],
        help="T·∫£i l√™n file handbook ƒë·ªãnh d·∫°ng JSON, PDF"
    )

    if uploaded_file:
        st.success(f"ƒê√£ t·∫£i file {uploaded_file.name} th√†nh c√¥ng !")
        file_details = {
            "filename": uploaded_file.name,
            "filetype": uploaded_file.type,
            "filesize": f"{uploaded_file.size / 1024:.2f} KB"
        }
        st.json(file_details)

        if st.button("X·ª≠ l√Ω v√† l∆∞u d·ªØ li·ªáu v√†o Milvus", type="primary"):
            if not collection_name:
                st.error("Vui l√≤ng nh·∫≠p t√™n collection tr∆∞·ªõc khi ti·∫øp t·ª•c.")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω v√† l∆∞u d·ªØ li·ªáu v√†o Milvus..."):
                try:
                    seed_milvus(
                        'http://localhost:19530',
                        collection_name,
                        uploaded_file,
                        use_vihuggingface=use_vihuggingface_embeddings
                    )
                    st.success(f"ƒê√£ t·∫£i d·ªØ li·ªáu th√†nh c√¥ng v√†o collection '{collection_name}'!")

                except Exception as e:
                    st.error(f"L·ªói khi l∆∞u d·ªØ li·ªáu v√†o Milvus: {e}")
    return collection_name

def handle_local_file(use_vihuggingface_embeddings: bool):
    st.info("HEHE chua lam xong ham nay")

def setup_chat_interface(model_choice: str):
    st.title("AI Assistant")

    if model_choice == "Qwen3-4B-Instruct (local)":
        st.caption("ƒêang s·ª≠ d·ª•ng m√¥ h√¨nh Qwen3-4B-Instruct ch·∫°y local.")
    else:
        st.caption("ƒêang s·ª≠ d·ª•ng m√¥ h√¨nh OpenAI GPT-5-nano.")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")
    
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n ?"}
        ]
    
    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs

def handle_user_input(msgs, agent_executor):
    prompt = st.chat_input("H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ s·ªï tay sinh vi√™n Thu·ª∑ L·ª£i !")
    
    if prompt:  
        st.session_state.messages.append({"role": "human", "content": prompt})
        with st.chat_message("human"):
            st.write(prompt)
        msgs.add_user_message(prompt)

        with st.chat_message("assistant"):
            # Show thinking process in an expander
            with st.expander(">> Xem qu√° tr√¨nh x·ª≠ l√Ω", expanded=False):
                st_callback = StreamlitCallbackHandler(st.container())
                
                chat_history = msgs.messages[:-1]
                response = agent_executor.invoke(
                    {"input": prompt, "chat_history": chat_history},
                    {"callbacks": [st_callback]}
                )
            
            answer = response["output"]
            st.session_state.messages.append({"role": "assistant", "content": answer})
            msgs.add_ai_message(answer)
            st.write(answer)
            release_memory()

#  === MAIN FUNCTION ===
def main():
    initialize_app()
    model_choice, collections_to_query = setup_sidebar()
    msgs = setup_chat_interface(model_choice)

    if model_choice == "OpenAI GPT-5-nano":
        retriever = get_openai_retriever(collections_to_query)
        agent_executor = get_openai_agent(retriever)
    else:
        retriever = get_ollama_retriever(collections_to_query)
        agent_executor = get_ollama_agent(retriever)

    handle_user_input(msgs, agent_executor)

if __name__ == '__main__':
    main()